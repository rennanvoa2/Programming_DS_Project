{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming for Data Science\n",
    "## NOVA IMS\n",
    "\n",
    "#### Group Name/Number: <br>\n",
    "1. María Fernanda Restrepo Suescún / <span style=\"color:blue\"> Num: 20190679 <br>\n",
    "2. Rennan Valadares Ornelas Araújo / <span style=\"color:blue\"> Num: 20190146 <br>\n",
    "3. Henrique Confraria Varatojo Caldas Januário / <span style=\"color:blue\"> Num: 20190438 <br>\n",
    "4. Chanmolyta Tham  / <span style=\"color:blue\"> Num: 20190927 <br>\n",
    "\n",
    "\n",
    "### Group Project Summary\n",
    "This project report aims to present insight into a potential client and he is willing to know where he should invest next, particularly in the tourism sector. The key recommendations to business's need based on the cost and risk analysis, and other factors including socio-economic, political, and technology potential, etc. We will get into this in the plots, where countries with high-growth, high-risk, high-arrivals, high competition and On-the-Growth with medium/low risk. \n",
    "\n",
    "The dataset we used is International Tourism by Number of Arrivals by country, the world, data sourced from the World Tourism Organisation (WTO) and the World Bank's databank.\n",
    "\n",
    "\n",
    "Our group starts checking the provided dataset, tried to understand the regionals and countries' perspectives and its currents status, countries made up to the top of the list as well as countries had the remarkable growth in the past 10 years. Also, businesses' ideas in the Tourism sector, that link within/or the rest of the regions, only within countries kind of business.\n",
    "\n",
    "***We used GitHub from the beginning to version control. You can check our project on the following link: https://github.com/rennanvoa2/Programming_DS_Project***\n",
    "\n",
    "The following report presents a step-by-step process from data preparation to data visualization and presenting. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Cleaning \n",
    "\n",
    "#### 1.1 Importing libraries\n",
    "\n",
    "We used Scikit Learn for Machine Learning models for filling the NaNs with a regression, pandas, and NumPy for managing data, MatPlotLib and Seaborn for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "#Imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from copy import deepcopy\n",
    "from sklearn.svm import LinearSVR\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Functions for Data Preparation\n",
    "\n",
    "##### 1.2.1 regressor_test\n",
    "We started filling our NaNs with the Average of each column. Then we figured out that we could create a regressor for having better results.<br>\n",
    "The main goal for the regressor_test function is to realize which of the 3 Machine Learning models is the best one for filling our NaN data. To accomplish this the regressor_test function create's 3 different regressor models ***for each column*** following these steps:<br><br>\n",
    "<span style=\"color:blue\">\n",
    "1 - Split the data in Train and Test (80% and 20% respectively)<br>\n",
    "2 - Create 3 different regressors: KNeighborsRegressor, LinearRegression, and LinearSVR.<br>\n",
    "3 - Train each regressor with Train data.<br>\n",
    "4 - Fill other columns (instead of the one in the loop) with the row average. We decided to do it because we cant have NaNs on the other columns.<br>\n",
    "5 - Make the Predictions on the test set.<br>\n",
    "6 - Check the Mean Squared Error for each model and append it to its list.<br>\n",
    "7 - Append the best model for each column on the MSE list.<br>\n",
    "8 - Count number of times of each model name appears on the list and return the highest.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressor_test(complete,incomplete,years):\n",
    "    '''This function test 3 diferent Machine Learning models (KNeighborsRegressor, LinearRegression, LinearSVR)\n",
    "    returning the best one.\n",
    "    It will return a string with the name of the best model.\n",
    "    Complete: Pandas Dataset with complete data.\n",
    "    Incomplete: Pandas Dataset with NaNs.\n",
    "    years: List with the name of the columns'''\n",
    "    kn_errors = []\n",
    "    linear_errors = []\n",
    "    svr_errors = []    \n",
    "    \n",
    "    for i in years:\n",
    "        \n",
    "        #Step 1\n",
    "        X_train, X_test, y_train, y_test = train_test_split(complete.loc[:,complete.columns != i].values,\n",
    "                                                            complete.loc[:,i].values, test_size = 0.2, random_state = 0)\n",
    "        \n",
    "        #Step 2\n",
    "        regressor1 = KNeighborsRegressor(2, \n",
    "                                       weights ='distance', \n",
    "                                       metric = 'euclidean')\n",
    "        regressor2= LinearRegression()\n",
    "        regressor3=LinearSVR()\n",
    "        \n",
    "        #Step 3\n",
    "        trained_model1 = regressor1.fit(X_train, \n",
    "                                 y_train)\n",
    "        trained_model2 = regressor2.fit(X_train, \n",
    "                                 y_train)\n",
    "        trained_model3 = regressor3.fit(X_train, \n",
    "                                 y_train)  \n",
    "        \n",
    "        #Step 4\n",
    "        incomplete_2 = deepcopy(incomplete)\n",
    "        incomplete_2.loc[:, incomplete.columns != i] = incomplete_2.loc[:, \n",
    "                                incomplete.columns != i].apply(lambda row: row.fillna(row.mean()), axis=1)\n",
    "\n",
    "        #Step 5\n",
    "        y_pred1 = regressor1.predict(X_test)\n",
    "        y_pred2 = regressor2.predict(X_test)\n",
    "        y_pred3 = regressor3.predict(X_test)\n",
    "        \n",
    "        #Step 6\n",
    "        kn_errors.append(mean_squared_error(y_test, y_pred1))\n",
    "        linear_errors.append(mean_squared_error(y_test, y_pred2))\n",
    "        svr_errors.append(mean_squared_error(y_test, y_pred3))\n",
    "        \n",
    "        \n",
    "         \n",
    "    MSE= []\n",
    "    \n",
    "    #Step 7\n",
    "    for i in range(0, len(complete.loc[:,'2007':'2017'].columns)):\n",
    "        l = []\n",
    "        l.extend((kn_errors[i], linear_errors[i], svr_errors[i]))\n",
    "        \n",
    "        if min(l) == kn_errors[i]:\n",
    "            MSE.append(\"KNN\")\n",
    "        elif min(l) == linear_errors[i]:\n",
    "            MSE.append(\"Linear\")\n",
    "        elif min(l) == svr_errors[i]:\n",
    "            MSE.append(\"SVR\")\n",
    "\n",
    "    #Step 8\n",
    "    print(\"KNN =\",MSE.count(\"KNN\"),'\\nLinear =',MSE.count(\"Linear\") ,'\\nSVR =',MSE.count(\"SVR\"))\n",
    "    return max(set(MSE), key = MSE.count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.2 apply_regressor\n",
    "The apply_regressor function uses the string return of regressor_test function to apply the chosen model. First, we check which model was chosen on the choice variable, following for almost the same steps of regressor_test instead for the last part:<br><br>\n",
    "<span style=\"color:blue\">\n",
    "1 - Check which regressor we will use and create the regressor.\n",
    "2 - Split the data in Train and Test (80% and 20% respectively)<br>\n",
    "3 - Train each regressor with Train data.<br>\n",
    "4 - Fill other columns (instead of the one in the loop) with the row average. We decided to do it because we cant have NaNs on the other columns.<br>\n",
    "5 - Make the Predictions on the test set.<br>\n",
    "6 and 7 - Fill the NaNs in incomplete and pass it to the main dataset.<br>\n",
    "8 - Return the dataset filled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_regressor(choice,complete, incomplete,dataset,years):\n",
    "    ''' The main goal of apply_regressor function is to apply the model chosen on regressor_test on the dataset.\n",
    "    choice: String with name of the best model.\n",
    "    Complete: Pandas Dataset with complete data.\n",
    "    Incomplete: Pandas Dataset with NaNs.\n",
    "    dataset: The main dataset with all the data (the one you want to fill the NaNs).\n",
    "    years: List with the name of the columns'''\n",
    "    \n",
    "    #Step 1\n",
    "    for i in years:\n",
    "        if choice == 'KNN':\n",
    "            regressor = KNeighborsRegressor(2, \n",
    "                                            weights ='distance', \n",
    "                                            metric = 'euclidean')\n",
    "        elif choice == 'SVR':\n",
    "            regressor = LinearSVR()\n",
    "        elif choice == 'Linear':\n",
    "            regressor = LinearRegression()\n",
    "        #Step 2    \n",
    "        X_train, X_test, y_train, y_test = train_test_split(complete.loc[:,complete.columns != i].values,\n",
    "                                                            complete.loc[:,i].values, test_size = 0.2, random_state = 0)\n",
    "        #Step 3\n",
    "        trained_model = regressor.fit(X_train, \n",
    "                                 y_train)\n",
    "        #Step 4\n",
    "        incomplete_2 = deepcopy(incomplete)\n",
    "        incomplete_2.loc[:, incomplete.columns != i] = incomplete_2.loc[:, \n",
    "                                incomplete.columns != i].apply(lambda row: row.fillna(row.mean()), axis=1)\n",
    "        #Step 5\n",
    "        prediction = trained_model.predict(incomplete_2.loc[:,incomplete_2.columns != i])\n",
    "        temp_df = pd.DataFrame(prediction.reshape(-1,1), columns = [i])\n",
    "        \n",
    "        #Step 6\n",
    "        #now we are filling incomplete \n",
    "        for index in range(len(temp_df)):\n",
    "            if np.isnan(incomplete[i][index]):\n",
    "                incomplete[i][index] = temp_df[i][index]\n",
    "\n",
    "\n",
    "    #Step 7\n",
    "    #and filling the nan's on the main dataframe\n",
    "    dataset.loc[:,'2007':'2017'] = pd.concat([complete, incomplete])\n",
    "    \n",
    "    #Step 8\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.3 download_clean_data\n",
    "This function download the dataset and metadata, merge and clean it.<br>\n",
    "***Approach:***\n",
    "\n",
    "- Load Data.\n",
    "- Select Country Name, Country Code and the 11 last years (2017 is used just to check the growth from 2017 to 2018);\n",
    "- Join with metadata on Country Code;\n",
    "- Set Country Name as Index;\n",
    "- Removing the 'Regions' (Will just use countries)\n",
    "- Drop rows with more than 3 NaNs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_clean_data(data_url, metadata_url):\n",
    "    ''' Download the data from the given URL, merge with metadata, drop rows with more than 3 NaNs.\n",
    "    data_url: Pandas main dataset URL\n",
    "    metadata_url: Pandas metadata dataset URL '''\n",
    "    \n",
    "    #Read the dataset CSV\n",
    "    data=pd.read_csv(data_url, header=2)\n",
    "    \n",
    "    #Select the columns with usefull data we need 2007 here to calculate the grow from 2007 to 2008\n",
    "    data=data[['Country Name', 'Country Code','2007' , '2008', '2009', '2010', '2011',\n",
    "                       '2012', '2013', '2014', '2015', '2016', '2017']]\n",
    "        \n",
    "        #Load metadata CSV\n",
    "    metadata_country=pd.read_csv(metadata_url, header=0) \n",
    "    \n",
    "        #Merge data CSV with Metadata CSV\n",
    "    data_df=data.merge(metadata_country, on='Country Code', how='left') \n",
    "        \n",
    "        #Set Country Name as Index\n",
    "    new_index = data_df['Country Name']\n",
    "    data_df.set_index(new_index,inplace=True )\n",
    "    \n",
    "        #Create a column named Is_Country for later removing the \"areas\" like asia\n",
    "    data_df['Is_Country'] = data_df['Region'].notnull()\n",
    "    \n",
    "        #Drop unnecessary columns\n",
    "    data_df.drop(['Country Name', 'Unnamed: 5', 'Region', 'IncomeGroup', 'SpecialNotes',\n",
    "                  'TableName'], inplace=True, axis=1)\n",
    "    \n",
    "        #drop the 'areas'\n",
    "    data_df = data_df[data_df.Is_Country != False]\n",
    "    \n",
    "        #drop the Is_Country column becouse we dont need it anymore\n",
    "    data_df.drop('Is_Country', inplace=True, axis=1)\n",
    "    \n",
    "        #drop rows with 3 or more NANs values\n",
    "    data_df.dropna(thresh=(len(data_df.loc[:,'2008':'2017'].columns) - 1), inplace=True, axis=0)\n",
    "    \n",
    "\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Get URL's\n",
    "We work with provided data set indicator name: international tourism, the number of arrivals, metadata country and another indicator name: international tourism, receipts (current US$) *changed to Income in the report*, from the same the world bank databank. \n",
    "\n",
    "We use GitHub for sharing files, brief note and set our next goals and importing libraries for data analysis and visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the URI from the github files (Number of Arrivels and Income)\n",
    "#If you have any problem with this code, just go to https://github.com/rennanvoa2/Programming_DS_Project\n",
    "#Copy the raw link from each csv file and change the strings below\n",
    "income_url = 'https://raw.githubusercontent.com/rennanvoa2/Programming_DS_Project/master/Income.csv?token=AGBCKJWJQHIXYVN4UUSPPKC5ZKGIY'\n",
    "arrival_url = 'https://raw.githubusercontent.com/rennanvoa2/Programming_DS_Project/master/International%20Arrivals.csv?token=AGBCKJSNMRBUHCWG5W7FRG25ZKGNC'\n",
    "metadata_url = 'https://raw.githubusercontent.com/rennanvoa2/Programming_DS_Project/master/Metadata_Country.csv?token=AGBCKJSUCSZUTPS4I5AECTS5ZKGOC'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Load and Clean the Data\n",
    "##### 1.3.1 Apply download_clean_data function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrivals_df = download_clean_data(arrival_url,metadata_url)\n",
    "income_df=download_clean_data(income_url,metadata_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.3.2 Get the intersection\n",
    "Dif_row is a merged data frame that only has common rows of information, merged by Country Code.<br>\n",
    "We used this approach because we dropped some countries from both data frames (Arrivals and Income) when removed NaNs, and after that, the data frames have different countries. We decided to use just the countries that are in both data frames.<br>\n",
    "Both data frames have the same 117 observations and 11 years (2007-2017) for next stage of Data Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of the names of the columns we are going to use\n",
    "years = ['2007','2008','2009','2010','2011','2012','2013','2014', '2015','2016', '2017']\n",
    "\n",
    "#dif_row is a merged dataframe that only has common rows of information, merged by Country Code\n",
    "dif_row=pd.merge(arrivals_df,income_df,on='Country Code',left_index=True)\n",
    "dif_row=dif_row.drop(['Country Code'], axis=1)\n",
    "\n",
    "#We now have the same countries on both lists\n",
    "arrivals_df=dif_row.iloc[:,0:int(len(dif_row.columns)/2)]\n",
    "income_df=dif_row.iloc[:,int(len(dif_row.columns)/2):]\n",
    "\n",
    "arrivals_df.head()\n",
    "#renaming columns with the correct names\n",
    "arrivals_df.columns = years\n",
    "income_df.columns = years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 1.4 Split Arrival Data\n",
    "Here we split the Arrivals data frame into data_arrivals_complete (rows without NaNs) and data_arrivals_incomplete (rows with at least one NaN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    " #preparing arrival data for regressors\n",
    "data_arrivals_complete = pd.DataFrame()\n",
    "data_arrivals_incomplete = arrivals_df[arrivals_df.isna().any(axis=1)]\n",
    "data_arrivals_complete = arrivals_df[~arrivals_df.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 Apply the best regressor on Arrival data frame.\n",
    "We set the name of the best regressor on the variable choice (String). Then we apply this regressor with apply_regressor function and save the return data frame on arrivals_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN = 0 \n",
      "Linear = 8 \n",
      "SVR = 3\n",
      "Best type of regression to be used for arrivals prediction-> Linear\n"
     ]
    }
   ],
   "source": [
    "#applying created functions\n",
    "choice= regressor_test(data_arrivals_complete,data_arrivals_incomplete,years)\n",
    "print('Best type of regression to be used for arrivals prediction->',choice)\n",
    "arrivals_df=apply_regressor(choice,data_arrivals_complete,data_arrivals_incomplete,arrivals_df,years)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 Split Income Data\n",
    "Here we split the Income data frame into data_income_complete (rows without NaNs) and data_income_incomplete (rows with at least one NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing income data for regressors\n",
    "data_income_complete = pd.DataFrame()\n",
    "data_income_incomplete = income_df[income_df.isna().any(axis=1)]\n",
    "data_income_complete = income_df[~income_df.isna().any(axis=1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7 Apply the best regressor on Income data frame..\n",
    "After running the regressor_test function for the Income data frame, we realized that the regressor wasn't performing well. When we apply it, some predictions returned negative numbers. Then we decided to use the second best, which was SVR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN = 1 \n",
      "Linear = 5 \n",
      "SVR = 5\n",
      "Best type of regression to be used for income prediction -> Linear\n"
     ]
    }
   ],
   "source": [
    "#applying created functions\n",
    "choice= regressor_test(data_income_complete,data_income_incomplete,years)\n",
    "print('Best type of regression to be used for income prediction ->',choice)\n",
    "income_df=apply_regressor('SVR', data_income_complete, data_income_incomplete, income_df, years)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Preparation\n",
    "#### 2.1 common_fm\n",
    "\n",
    "The goal for this function is to create commom features in both datasets. We are using 4 commom features: \n",
    "- Avarage in 10 years(Avg_10_Years) - Mean of the row;\n",
    "- %Avarage(%Avg) - divide each value of Avarage in 10 years for the sum of the column;\n",
    "- Growth in 10 Years(Growth10ys)  - (Data from 2017 - data from 2008) / data from 2008;\n",
    "- %Growth(% growth) - Divide each value of Growth in 10 years for the sum of the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_fm(data_df):\n",
    "    ''' This function create features on the given dataset. \n",
    "    Features created: Avarage in 10 years(Avg_10_Years), Growth in 10 Years(Growth10ys), %Growth(% growth) and %Avarage(%Avg).\n",
    "    data_df: Pandas Dataset'''\n",
    "        #create feature Avarage in Last 10 Years\n",
    "    data_df['Avg_10_Years'] = data_df.loc[:,'2008':'2017'].mean(axis=1)\n",
    "    \n",
    "        #Create the feature Growth in 10 years\n",
    "    data_df['Growth10ys']=(data_df['2017']/data_df['2008']-1)\n",
    "    \n",
    "    \n",
    "    annual_growth = pd.DataFrame(index=data_df.index.values)\n",
    "    \n",
    "        #Fill the growth of each year in annual_arrival_growth dataframe\n",
    "    for i in arrivals_df.loc[:,'2008':'2017'].columns:\n",
    "        annual_growth[i] = (data_df[i] - data_df[str(int(i)-1)]) / data_df[str(int(i)-1)]\n",
    "       \n",
    "        #New Growth metric, becouse the last one wasnt good.\n",
    "    data_df[\"AVG_Growth\"] =  annual_growth.mean(axis=1)\n",
    "    \n",
    "        #sort by the best avarage arrivals in the last 10 years\n",
    "    data_df = data_df.sort_values('Avg_10_Years', ascending =False)\n",
    "\n",
    "        #divide each value of Growth in 10 years for the sum of the column\n",
    "    data_df['% growth'] = data_df['AVG_Growth'] / data_df['AVG_Growth'].sum()\n",
    "\n",
    "        #divide each value of Avarage in 10 years for the sum of the column\n",
    "    data_df['%Avg'] = data_df['Avg_10_Years'] / data_df['Avg_10_Years'].sum()\n",
    "\n",
    "\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Creating the new metrics\n",
    "We started by giving the same weigh for growth metrics and total number metrics, but the result showed the countries with higher growth in the last 10 years dominated the top 10 countries despite the number of the total number. Then we decided to give more weight to the total number. It makes sense to balance both features, but the total number should count more than growth for a safe result.\n",
    "We also created the feature AVG_expenditure_per_person, which was made dividing Average 10 years (income) for Average 10 years (arrivals), then we used this metric to create %Avg_Per_Person dividing each value for column total.\n",
    "On our last step, we create Growth x Average x Avg Exp feature, which balances %Growth, %Avarage and %Avg Exp Feature based on the wight's chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weigh's for metrics\n",
    "arrivals_total_number_weight = 3\n",
    "arrivals_growth_weight = 1\n",
    "income_total_number_weight = 3\n",
    "income_growth_weight = 1\n",
    "avg_per_person_weight = 1\n",
    "\n",
    "\n",
    "#for arrivals\n",
    "\n",
    "arrivals_df = common_fm(arrivals_df)\n",
    "\n",
    "#Calculate the avarage between Growth and Avarage Numbers of Arrivals\n",
    "arrivals_df['Growth x Average'] = (arrivals_growth_weight * arrivals_df['% growth'] +\n",
    "               (arrivals_total_number_weight * arrivals_df['%Avg'])) / (arrivals_total_number_weight + arrivals_growth_weight)\n",
    "\n",
    "#create a dataframe sorted by Growth X Avarage\n",
    "Arrivals_in_growth_vs_arrivals = arrivals_df.sort_values('Growth x Average', ascending=False)\n",
    "\n",
    "\n",
    "#for income\n",
    "\n",
    "income_df = common_fm(income_df)\n",
    "\n",
    "#Create Avarage expenditure per person\n",
    "income_df['AVG_expenditure_per_person'] = income_df['Avg_10_Years'] / arrivals_df['Avg_10_Years']\n",
    "\n",
    "\n",
    "#divide each value of Avarage per person for the sum of the column\n",
    "income_df['%Avg_Per_Person'] = income_df['AVG_expenditure_per_person'] / income_df['AVG_expenditure_per_person'].sum()\n",
    "\n",
    "#Calculate the avarage between Growth and Avarage Numbers of Arrivals\n",
    "income_df['Growth x Average x Avg Exp'] = (income_growth_weight * income_df['% growth'] +\n",
    "               (income_total_number_weight* income_df['%Avg']) + \n",
    "               avg_per_person_weight * income_df['%Avg_Per_Person']) / (income_total_number_weight + \n",
    "                                                income_growth_weight + avg_per_person_weight)\n",
    "\n",
    "#create a dataframe sorted by Growth X Avarage\n",
    "income_in_growth_vs_income = income_df.sort_values('Growth x Average x Avg Exp', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Drop Outliers\n",
    "We didn't want to drop many outliers, but Belarus and Congo was biasing our data. No matter what weight we chose for total number, Belarus and Congo were always listed. When we check their growth on the dataframe the difference between them and the others was too big, raising a red flag. Checking the metadata we realized that data before 2015 were adjusted to reflect the new denomination effective from July 1, thats why we decided to drop it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop Belarus, its an outlier in Arrivals Dataset\n",
    "Arrivals_in_growth_vs_arrivals = Arrivals_in_growth_vs_arrivals.drop(['Belarus'])\n",
    "\n",
    "#Drop Congo, its an outlier in income Dataset\n",
    "income_in_growth_vs_income = income_in_growth_vs_income.drop(['Congo, Dem. Rep.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Pick the top 10 of each data frame\n",
    "Using the Arrivals_in_growth_vs_arrivals and income_in_growth_vs_income features we selected the top 10 countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the best 10 results in Arrivals\n",
    "arrival_top_10 = Arrivals_in_growth_vs_arrivals.iloc[0:10,:]\n",
    "\n",
    "#get the best 10 results in income\n",
    "income_top_10 = income_in_growth_vs_income.iloc[0:10,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Plots\n",
    "\n",
    "Using Seaborn library we built the next bar charts showing the top countries with the biggest growth in arrivals, Income and a combination of both measures.\n",
    "#### 3.1 Arrivals top 10 Bar Chart\n",
    "\n",
    "This Bar Chart shows the 10 countries with the biggest average growth in Arrivals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Growth x Avarage'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2656\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2657\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2658\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Growth x Avarage'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-8f98a4cecbe1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Plot the Arrivals values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0msb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_color_codes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"pastel\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0msb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbarplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0marrival_top_10\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Growth x Avarage'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marrival_top_10\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marrival_top_10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'b'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Name the x axis and set its fontsize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2925\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2926\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2927\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2928\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2657\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2658\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2659\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2661\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Growth x Avarage'"
     ]
    }
   ],
   "source": [
    "# Set some initial parameters of the graph\n",
    "plt.rcParams['figure.figsize'] = (30, 10)\n",
    "sb.set(style=\"whitegrid\", font_scale=1.8)\n",
    "\n",
    "# Plot the Arrivals values\n",
    "sb.set_color_codes(\"pastel\")\n",
    "sb.barplot(y= arrival_top_10['Growth x Avarage'] , x = arrival_top_10.index.values, data = arrival_top_10, color='b')\n",
    "\n",
    "# Name the x axis and set its fontsize\n",
    "plt.xlabel('Top Countries', fontsize=25) \n",
    "\n",
    "# Name the y axis and set its fontsize\n",
    "plt.ylabel('Percentage of Growth', fontsize=25) \n",
    "  \n",
    "# Give a title and set its fontsize\n",
    "plt.title(\"Growth in Arrivals\", fontsize=25) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Income top 10 Bar Chart\n",
    "\n",
    "This Bar Chart shows the 10 countries with the biggest average growth in Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Growth x Avarage x Avg Exp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2656\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2657\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2658\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Growth x Avarage x Avg Exp'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-16da12dd1ce9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Plot the Income values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0msb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_color_codes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"muted\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0msb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbarplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mincome_top_10\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Growth x Avarage x Avg Exp'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mincome_top_10\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mincome_top_10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'b'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Name the x axis and set its fontsize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2925\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2926\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2927\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2928\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2657\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2658\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2659\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2661\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Growth x Avarage x Avg Exp'"
     ]
    }
   ],
   "source": [
    "# Set some initial parameters of the graph\n",
    "plt.rcParams['figure.figsize'] = (30, 10)\n",
    "sb.set(style=\"whitegrid\", font_scale=1.8)\n",
    "\n",
    "# Plot the Income values\n",
    "sb.set_color_codes(\"muted\")\n",
    "sb.barplot(y= income_top_10['Growth x Avarage x Avg Exp'] , x = income_top_10.index.values, data = income_top_10, color='b')\n",
    "\n",
    "# Name the x axis and set its fontsize\n",
    "plt.xlabel('Top Countries', fontsize=25) \n",
    "\n",
    "# Name the y axis and set its fontsize\n",
    "plt.ylabel('Percentage of Growth', fontsize=25) \n",
    "  \n",
    "# Give a title and set its fontsize\n",
    "plt.title(\"Growth in Income\", fontsize=25) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Growth in Income and Arrivals Bar Chart\n",
    "\n",
    "This graph is the combination of both Bar Charts above and shows the countries with the biggest growth in arrivals and income in the last years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list with unique countries in arrival_top_10 + income_top_10\n",
    "label_array = list(set(list(arrival_top_10.index.values)+ list(income_top_10.index.values)))\n",
    "\n",
    "#Create a dataframe with the sum of top arrivals + top income\n",
    "Arrivals_plus_income = pd.DataFrame(index=label_array)\n",
    "Arrivals_plus_income['Arrivals GxA'] = arrivals_df['Growth x Avarage']\n",
    "Arrivals_plus_income['Income GxA'] = income_df['Growth x Avarage x Avg Exp']\n",
    "\n",
    "#sort the DataFrame by income\n",
    "Arrivals_plus_income = Arrivals_plus_income.sort_values('Income GxA', ascending =False)\n",
    "\n",
    "\n",
    "# Set some initial parameters of the graph\n",
    "plt.rcParams['figure.figsize'] = (30, 10)\n",
    "sb.set(style=\"whitegrid\", font_scale=1.8)\n",
    "\n",
    "# Plot the Income values\n",
    "sb.set_color_codes(\"muted\")\n",
    "sb.barplot(y=Arrivals_plus_income['Income GxA'], x = Arrivals_plus_income.index.values, data = Arrivals_plus_income, color='b', label='Income')\n",
    "\n",
    "# Plot the Arrivals values\n",
    "sb.set_color_codes(\"pastel\")\n",
    "sb.barplot(y= Arrivals_plus_income['Arrivals GxA'] , x = Arrivals_plus_income.index.values, data = Arrivals_plus_income, color='b', bottom=Arrivals_plus_income['Income GxA'], label='Arrival')\n",
    "\n",
    "# Name the x axis and set its fontsize\n",
    "plt.xlabel('Top Countries', fontsize=25) \n",
    "\n",
    "# Name the y axis and set its fontsize\n",
    "plt.ylabel('Percentage of Growth', fontsize=25) \n",
    "  \n",
    "# Give a title and set its fontsize\n",
    "plt.title(\"Growth by Income and Arrivals\", fontsize=25) \n",
    "\n",
    "# Add the legend\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
